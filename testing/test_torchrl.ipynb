{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f0e6e5e",
   "metadata": {},
   "source": [
    "This notebook is to test Torch RL functions to make reward, action etc tracking easier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4d1b6a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Tensordict modules\n",
    "from tensordict.nn import set_composite_lp_aggregate, TensorDictModule, TensorDictSequential\n",
    "from tensordict import  TensorDictBase\n",
    "from torch import multiprocessing\n",
    "\n",
    "# Data collection\n",
    "from torchrl.collectors import SyncDataCollector\n",
    "from torch.distributions import Categorical\n",
    "from torchrl.data.replay_buffers import ReplayBuffer\n",
    "from torchrl.data.replay_buffers.samplers import SamplerWithoutReplacement\n",
    "from torchrl.data.replay_buffers.storages import LazyTensorStorage\n",
    "\n",
    "#Env\n",
    "from torchrl.envs import RewardSum, TransformedEnv, PettingZooWrapper, Compose, DoubleToFloat, StepCounter, ParallelEnv, EnvCreator, ExplorationType, set_exploration_type\n",
    "\n",
    "# Utils\n",
    "from torchrl.envs.utils import check_env_specs\n",
    "\n",
    "# Multi-agent network\n",
    "from torchrl.modules import MultiAgentMLP, ProbabilisticActor, TanhNormal\n",
    "\n",
    "# Loss\n",
    "from torchrl.objectives import ClipPPOLoss, ValueEstimators\n",
    "\n",
    "# Utils\n",
    "torch.manual_seed(0)\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from til_environment import gridworld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "edc041cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_rewards = {\n",
    "    RewardNames.GUARD_CAPTURES: 100,     \n",
    "    RewardNames.SCOUT_CAPTURED: -100,    \n",
    "    RewardNames.SCOUT_RECON: 2,         \n",
    "    RewardNames.SCOUT_MISSION: 10,      \n",
    "    RewardNames.WALL_COLLISION: -5,       # experiemnt with this \n",
    "    RewardNames.STATIONARY_PENALTY: -1, \n",
    "    RewardNames.SCOUT_STEP: -0.1,         # small step penalty to encourage efficiency\n",
    "    RewardNames.GUARD_STEP: -0.1,         # same for guards \n",
    "}\n",
    "\n",
    "RENDER_MODE = 'rgb_array'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "12eb34f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nReward sum transformation => \\n\\nreward_sum = RewardSum(\\n    in_keys=[(\"player\", \"reward\")],\\n    out_keys=[(\"player\", \"episode_reward\")]\\n)\\n\\nReads from td[player][reward] => transforms to => td[player][episode_reward]\\n'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "by default AEC env, so agents take one turn at a time and perform at their action \n",
    "\n",
    "for torchRL, would need to train with parallel, meaning all agents cast their moves at the same time, so wrap to parllelENV that is given \n",
    "\n",
    "PettingZooWrapper => wraps the parllel env into a framework that is compatible with RL algorithms such as MAPPO and transforms \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def make_env():\n",
    "    base_env = gridworld.parallel_env(\n",
    "        render_mode=RENDER_MODE,\n",
    "        debug=False,\n",
    "        rewards_dict=custom_rewards,\n",
    "        novice=False\n",
    "    )\n",
    "\n",
    "    # Wrap with PettingZooWrapper to make it TorchRL-compatible\n",
    "    wrapped_env = PettingZooWrapper(\n",
    "        env=base_env,\n",
    "        categorical_actions=False,  # one-hot encoding \n",
    "        done_on_any=True            # end episode if any agent \n",
    "    )\n",
    "\n",
    "    # general transforms (per-agent reward tracking, float conversion, step count)\n",
    "    transformed_env = TransformedEnv(\n",
    "        wrapped_env,\n",
    "        Compose(\n",
    "            RewardSum(\n",
    "                in_keys=[(\"player\", \"reward\")],           # works across all agents\n",
    "                out_keys=[(\"player\", \"episode_reward\")]\n",
    "            ),\n",
    "            DoubleToFloat(),                              # ensure obs are float32\n",
    "            StepCounter()                                 # add step tracking\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return transformed_env\n",
    "\n",
    "\"\"\"\n",
    "Reward sum transformation => \n",
    "\n",
    "reward_sum = RewardSum(\n",
    "    in_keys=[(\"player\", \"reward\")],\n",
    "    out_keys=[(\"player\", \"episode_reward\")]\n",
    ")\n",
    "\n",
    "Reads from td[player][reward] => transforms to => td[player][episode_reward]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7c57b14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ParallelEnv(\n",
    "    num_workers=1, # for the sake of simplicity -> put 1 env for now \n",
    "    create_env_fn=EnvCreator(make_env),\n",
    "    serial_for_single=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b604b5a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_keys: [('player', 'action')]\n",
      "reward_keys: [('player', 'reward')]\n",
      "done_keys: ['done', 'terminated', 'truncated', ('player', 'done'), ('player', 'terminated'), ('player', 'truncated')]\n",
      "Action Spec: Composite(\n",
      "    player: Composite(\n",
      "        action: OneHot(\n",
      "            shape=torch.Size([1, 4, 5]),\n",
      "            space=CategoricalBox(n=5),\n",
      "            device=cpu,\n",
      "            dtype=torch.int64,\n",
      "            domain=discrete),\n",
      "        device=cpu,\n",
      "        shape=torch.Size([1, 4])),\n",
      "    device=cpu,\n",
      "    shape=torch.Size([1]))\n",
      "Observation Spec: Composite(\n",
      "    player: Composite(\n",
      "        observation: BoundedDiscrete(\n",
      "            shape=torch.Size([1, 4, 572]),\n",
      "            space=ContinuousBox(\n",
      "                low=Tensor(shape=torch.Size([1, 4, 572]), device=cpu, dtype=torch.int64, contiguous=True),\n",
      "                high=Tensor(shape=torch.Size([1, 4, 572]), device=cpu, dtype=torch.int64, contiguous=True)),\n",
      "            device=cpu,\n",
      "            dtype=torch.int64,\n",
      "            domain=discrete),\n",
      "        episode_reward: UnboundedContinuous(\n",
      "            shape=torch.Size([1, 4, 1]),\n",
      "            space=ContinuousBox(\n",
      "                low=Tensor(shape=torch.Size([1, 4, 1]), device=cpu, dtype=torch.float32, contiguous=True),\n",
      "                high=Tensor(shape=torch.Size([1, 4, 1]), device=cpu, dtype=torch.float32, contiguous=True)),\n",
      "            device=cpu,\n",
      "            dtype=torch.float32,\n",
      "            domain=continuous),\n",
      "        device=cpu,\n",
      "        shape=torch.Size([1, 4])),\n",
      "    step_count: BoundedDiscrete(\n",
      "        shape=torch.Size([1, 1]),\n",
      "        space=ContinuousBox(\n",
      "            low=Tensor(shape=torch.Size([1, 1]), device=cpu, dtype=torch.int64, contiguous=True),\n",
      "            high=Tensor(shape=torch.Size([1, 1]), device=cpu, dtype=torch.int64, contiguous=True)),\n",
      "        device=cpu,\n",
      "        dtype=torch.int64,\n",
      "        domain=discrete),\n",
      "    device=cpu,\n",
      "    shape=torch.Size([1]))\n",
      "Reward Spec: Composite(\n",
      "    player: Composite(\n",
      "        reward: UnboundedContinuous(\n",
      "            shape=torch.Size([1, 4, 1]),\n",
      "            space=ContinuousBox(\n",
      "                low=Tensor(shape=torch.Size([1, 4, 1]), device=cpu, dtype=torch.float32, contiguous=True),\n",
      "                high=Tensor(shape=torch.Size([1, 4, 1]), device=cpu, dtype=torch.float32, contiguous=True)),\n",
      "            device=cpu,\n",
      "            dtype=torch.float32,\n",
      "            domain=continuous),\n",
      "        device=cpu,\n",
      "        shape=torch.Size([1, 4])),\n",
      "    device=cpu,\n",
      "    shape=torch.Size([1]))\n",
      "Done Spec: Composite(\n",
      "    done: Categorical(\n",
      "        shape=torch.Size([1, 1]),\n",
      "        space=CategoricalBox(n=2),\n",
      "        device=cpu,\n",
      "        dtype=torch.bool,\n",
      "        domain=discrete),\n",
      "    terminated: Categorical(\n",
      "        shape=torch.Size([1, 1]),\n",
      "        space=CategoricalBox(n=2),\n",
      "        device=cpu,\n",
      "        dtype=torch.bool,\n",
      "        domain=discrete),\n",
      "    truncated: Categorical(\n",
      "        shape=torch.Size([1, 1]),\n",
      "        space=CategoricalBox(n=2),\n",
      "        device=cpu,\n",
      "        dtype=torch.bool,\n",
      "        domain=discrete),\n",
      "    player: Composite(\n",
      "        done: Categorical(\n",
      "            shape=torch.Size([1, 4, 1]),\n",
      "            space=CategoricalBox(n=2),\n",
      "            device=cpu,\n",
      "            dtype=torch.bool,\n",
      "            domain=discrete),\n",
      "        terminated: Categorical(\n",
      "            shape=torch.Size([1, 4, 1]),\n",
      "            space=CategoricalBox(n=2),\n",
      "            device=cpu,\n",
      "            dtype=torch.bool,\n",
      "            domain=discrete),\n",
      "        truncated: Categorical(\n",
      "            shape=torch.Size([1, 4, 1]),\n",
      "            space=CategoricalBox(n=2),\n",
      "            device=cpu,\n",
      "            dtype=torch.bool,\n",
      "            domain=discrete),\n",
      "        device=cpu,\n",
      "        shape=torch.Size([1, 4])),\n",
      "    device=cpu,\n",
      "    shape=torch.Size([1]))\n"
     ]
    }
   ],
   "source": [
    "# checking if the added functions appear as keys\n",
    "\n",
    "print(\"action_keys:\", env.action_keys)\n",
    "print(\"reward_keys:\", env.reward_keys)\n",
    "print(\"done_keys:\", env.done_keys)\n",
    "\n",
    "print(\"Action Spec:\", env.action_spec)\n",
    "print(\"Observation Spec:\", env.observation_spec)\n",
    "print(\"Reward Spec:\", env.reward_spec)\n",
    "print(\"Done Spec:\", env.done_spec)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd0befb",
   "metadata": {},
   "source": [
    "### Decoding the above tensordict (dictionary of tensors)\n",
    "\n",
    "```\n",
    "action_keys: [('player', 'action')]\n",
    "\n",
    "reward_keys: [('player', 'reward')]\n",
    "\n",
    "done_keys: ['done', 'terminated', 'truncated', ('player', 'done'), ('player', 'terminated'), ('player', 'truncated')]\n",
    "\n",
    "Action Spec: Composite(\n",
    "    player: Composite(\n",
    "        action: OneHot(\n",
    "            shape=torch.Size([1, 4, 5]),\n",
    "            space=CategoricalBox(n=5),\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "Observation Spec: Composite(\n",
    "    player: Composite(\n",
    "        observation: BoundedDiscrete(\n",
    "            shape=torch.Size([1, 4, 572]),\n",
    "        )\n",
    "        episode_reward: UnboundedContinuous(\n",
    "            shape=torch.Size([1, 4, 1]),\n",
    "        )\n",
    "    step_count: BoundedDiscrete(\n",
    "        shape=torch.Size([1, 1]),\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "Reward Spec: Composite(\n",
    "    player: Composite(\n",
    "        reward: UnboundedContinuous(\n",
    "            shape=torch.Size([1, 4, 1]),\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "Done Spec: Composite(\n",
    "    done: Categorical(\n",
    "        shape=torch.Size([1, 1]),\n",
    "        )\n",
    "    )\n",
    "    terminated: Categorical(\n",
    "        shape=torch.Size([1, 1]),\n",
    "    )\n",
    "    truncated: Categorical(\n",
    "        shape=torch.Size([1, 1]),\n",
    "    )\n",
    "    player: Composite(\n",
    "        done: Categorical(\n",
    "            shape=torch.Size([1, 4, 1]),\n",
    "        terminated: Categorical(\n",
    "            shape=torch.Size([1, 4, 1]),\n",
    "        )\n",
    "    )\n",
    "        truncated: Categorical(\n",
    "            shape=torch.Size([1, 4, 1]),\n",
    "        )\n",
    "    )\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2ffa27f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nflat-keys => top layer => such as 'player', 'done'...\\n\\n\\nto access nested infp => td['player']['done']\\n\\n\""
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ### Example of schema \n",
    "\n",
    "# TensorDict({\n",
    "#     'player': TensorDict({\n",
    "#         'observation': Tensor(shape=[1, 4, 572], dtype=torch.int64),\n",
    "#         'action': Tensor(shape=[1, 4, 5], dtype=torch.int64),        # one-hot\n",
    "#         'reward': Tensor(shape=[1, 4, 1], dtype=torch.float32),      # per-agent reward\n",
    "#         'episode_reward': Tensor(shape=[1, 4, 1], dtype=torch.float32),  # cumulative reward\n",
    "#         'done': Tensor(shape=[1, 4, 1], dtype=torch.bool),           # per-agent done\n",
    "#         'terminated': Tensor(shape=[1, 4, 1], dtype=torch.bool),     # true if done normally\n",
    "#         'truncated': Tensor(shape=[1, 4, 1], dtype=torch.bool),      # true if cutoff\n",
    "#     }),\n",
    "#     'step_count': Tensor(shape=[1, 1], dtype=torch.int64),           # steps taken in episode\n",
    "#     'done': Tensor(shape=[1, 1], dtype=torch.bool),                  # global done\n",
    "#     'terminated': Tensor(shape=[1, 1], dtype=torch.bool),            # global terminated\n",
    "#     'truncated': Tensor(shape=[1, 1], dtype=torch.bool),             # global truncated\n",
    "# })\n",
    "\n",
    "\n",
    "# consider a single rollout \n",
    "# td = {\n",
    "#   'player': {\n",
    "#     'observation': tensor([\n",
    "#         [  \n",
    "#             [...572 obs for player_0...],\n",
    "#             [...572 obs for player_1...],\n",
    "#             [...572 obs for player_2...],\n",
    "#             [...572 obs for player_3...]\n",
    "#         ]\n",
    "#     ]),  # shape = [1, 4, 572]\n",
    "\n",
    "#     'action': tensor([\n",
    "#         [  # 1 env, 4 agents\n",
    "#             [0, 1, 0, 0, 0],  # player_0 chose action 1\n",
    "#             [1, 0, 0, 0, 0],  # player_1 chose action 0\n",
    "#             [0, 0, 1, 0, 0],  # player_2 chose action 2\n",
    "#             [0, 0, 0, 1, 0],  # player_3 chose action 3\n",
    "#         ]\n",
    "#     ]),  # shape = [1, 4, 5]\n",
    "\n",
    "#     'reward': tensor([\n",
    "#         [\n",
    "#             [1.0],  # player_0\n",
    "#             [0.5],  # player_1\n",
    "#             [0.0],  # player_2\n",
    "#             [0.0]   # player_3\n",
    "#         ]\n",
    "#     ]),  # shape = [1, 4, 1]\n",
    "\n",
    "#     'episode_reward': tensor([\n",
    "#         [\n",
    "#             [3.0],  # player_0 has earned 3.0 total so far\n",
    "#             [2.5],  # player_1\n",
    "#             [1.0],  # player_2\n",
    "#             [0.0]   # player_3\n",
    "#         ]\n",
    "#     ]),  # shape = [1, 4, 1]\n",
    "\n",
    "#     'done': tensor([\n",
    "#         [\n",
    "#             [False],\n",
    "#             [False],\n",
    "#             [False],\n",
    "#             [True]   # player_3 is done (captured)\n",
    "#         ]\n",
    "#     ]),  # shape = [1, 4, 1]\n",
    "\n",
    "#     'terminated': same as done\n",
    "#     'truncated': all False unless cutoff by max steps\n",
    "#   },\n",
    "\n",
    "#   'step_count': tensor([[5]]),   # environment has run 5 steps so far\n",
    "\n",
    "#   'done': tensor([[True]]),      # environment is done (because of `done_on_any`)\n",
    "#   'terminated': tensor([[True]]),\n",
    "#   'truncated': tensor([[False]])\n",
    "# }\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "flat-keys => top layer => such as 'player', 'done'...\n",
    "\n",
    "\n",
    "to access nested infp => td['player']['done']\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9136bf4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-27 17:00:39,240 [torchrl][INFO] check_env_specs succeeded!\n"
     ]
    }
   ],
   "source": [
    "check_env_specs(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15209f2d",
   "metadata": {},
   "source": [
    "PettingZoo environments are AEC but also supports a parallel_env\n",
    "\n",
    "1. wrap the parallel_env in a PettingZooWrapper, converting the PettingZoo format into a TorchRL-compatible TransformedEnv\n",
    "\n",
    "\n",
    "2. once wrapped TorchRL can access observations, actions, rewards, and done flags through a structured format called a TensorDict. \n",
    "\n",
    "3. roles are rotated dynamically every round, group based func like RewardSum fails, instead of complex dynamic role allocation, can try with flag based approaches first "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36afaa6",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "til",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
